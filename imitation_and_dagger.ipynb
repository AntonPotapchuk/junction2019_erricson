{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CACHE_DIR = \"./cache\"\n",
    "DATASET_DIR = os.path.join(CACHE_DIR, \"dataset\")\n",
    "IMIT_LEARNING_DATASET_DIR = os.path.join(DATASET_DIR, \"imitation_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "game_id = 46749\n",
    "with open(os.path.join(IMIT_LEARNING_DATASET_DIR, 'game_{0}_obs_all.pkl'.format(game_id)), 'rb') as f:\n",
    "    observations_all = pickle.load(f)\n",
    "with open(os.path.join(IMIT_LEARNING_DATASET_DIR, 'game_{0}_scores_all.pkl'.format(game_id)), 'rb') as f:\n",
    "    scores_all = pickle.load(f)\n",
    "with open(os.path.join(IMIT_LEARNING_DATASET_DIR, 'game_{0}_actions_all.pkl'.format(game_id)), 'rb') as f:\n",
    "    actions_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_all.shape\n",
    "scores_all.shape\n",
    "actions_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enum\n",
    "# class CarDirection(enum.Enum):\n",
    "#     north = 0\n",
    "#     east = 1\n",
    "#     south = 2\n",
    "#     west = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(actions_all.astype('int'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_pad_observations(obs, receptor_size=100):\n",
    "    npad_ = (receptor_size-obs.shape[1])//2 # make sure the receptive field is always 200\n",
    "    npads = ((0, 0), (npad_, npad_), (npad_, npad_), (0, 0))\n",
    "    return np.pad(obs, pad_width=npads, mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, BatchNormalization, Add\n",
    "from keras.models import Model\n",
    "\n",
    "def get_res_block(input):\n",
    "    # Res block 1        \n",
    "    x = Convolution2D(256, 3, padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = Convolution2D(256, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([input, x])\n",
    "    x = Activation('elu')(x)\n",
    "    return x\n",
    "\n",
    "def create_model(actions, input_shape=(100, 100, 8,)):\n",
    "    inp = Input(input_shape)\n",
    "    x = Convolution2D(256, 3, padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "\n",
    "    # 3 residual blocks\n",
    "    for i in range(3):\n",
    "        x = get_res_block(x)\n",
    "\n",
    "    # Output block\n",
    "    # Should be 2 filters\n",
    "    x = Convolution2D(2, 1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)   \n",
    "    x = Activation('elu')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    probs  = Dense(actions, activation='softmax', name='actions')(x)\n",
    "    #reward = Dense(1, activation='tanh', name='reward')(x)\n",
    "\n",
    "    model = Model(inputs = inp, outputs=probs)\n",
    "    return model\n",
    "\n",
    "model_resblock = create_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from client import CarDirection, Client\n",
    "from env import JunctionEnvironment\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Softmax\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "img_dim = observations_all[0].shape\n",
    "action_dim = len(np.unique(actions_all))\n",
    "steps = 1000\n",
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "\n",
    "#model from https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "\n",
    "def create_model_1():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 8, 8, border_mode='same',\n",
    "                            input_shape=img_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 8, 8))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(64, 8, 8, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 8, 8))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution2D(128, 8, 8, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(128, 8, 8))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(action_dim, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# model = create_model_1()\n",
    "model = model_resblock\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(lr=1e-4),\n",
    "              metrics=['sparse_categorical_crossentropy', 'accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "###{{{ TensorBoard Logging and CheckPoint Callbacks begin\n",
    "supervised_experiment = np.random.randint(0, 100000)\n",
    "tb_cb = TensorBoard(log_dir='./logs/supervised_{0}'.format(supervised_experiment), histogram_freq=0, batch_size=batch_size, \n",
    "                    write_graph=True, write_grads=False, write_images=False, \n",
    "                    embeddings_freq=0, embeddings_layer_names=None, \n",
    "                    embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "checkpoint_path=\"checkpoints/supervised/weights-improvement-{epoch:02d}-{val_loss:.2f}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "cp_cb = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "load_checkpoint = '' # example: 'weights-improvement-01-1.50.ckpt'\n",
    "\n",
    "if load_checkpoint != '':\n",
    "    load_path = os.path.join(checkpoint_dir, load_checkpoint)\n",
    "    model.load_weights(load_path)\n",
    "    print(\"Loading %s \" % load_path)\n",
    "###}}} TensorBoard Logging and CheckPoint Callbacks Ends\n",
    "\n",
    "model.fit(observations_all, actions_all,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          shuffle=True,              \n",
    "          validation_split=0.2,\n",
    "          callbacks=[tb_cb, cp_cb])\n",
    "\n",
    "# output_file = open('results.txt', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alg_astar import search\n",
    "\n",
    "class CurrentTarget:\n",
    "    def __init__(self):\n",
    "        self.current_target = None\n",
    "        \n",
    "def megaalg(self, obs):\n",
    "        car_x, car_y = np.where(obs[:,:,4])[0][0], np.where(obs[:,:,4])[1][0]\n",
    "        customer_positions = []\n",
    "        customer_dists = []\n",
    "        paths_to_clients = []\n",
    "        maze = 1-obs[:,:,0]\n",
    "        statuses = []\n",
    "\n",
    "        if obs[:,:,3].sum() > 0:\n",
    "            # go to destination\n",
    "            if self.current_target is None:\n",
    "                self.current_target = np.where(obs[:,:,3])[0][0], np.where(obs[:,:,3])[1][0]\n",
    "            x, y = self.current_target\n",
    "            path, _ = search(maze, 1, (car_x, car_y), (x ,y))\n",
    "            if not path:\n",
    "                self.current_target = None\n",
    "                return 4\n",
    "            target_cell = path[0]\n",
    "            if len(path)==1:\n",
    "                self.current_target = None\n",
    "        else:\n",
    "            # look for customer\n",
    "\n",
    "            if obs[:,:,1].sum() == 0:\n",
    "                return 4\n",
    "            coords = np.where(obs[:,:,1])\n",
    "\n",
    "            if self.current_target is None:\n",
    "                for i in range(len(coords[0])):\n",
    "                    x, y = coords[0][i], coords[1][i]\n",
    "                    customer_positions.append((x,y))\n",
    "                    dist = np.abs(car_x - x) + np.abs(car_y - y)\n",
    "                    customer_dists.append(dist)\n",
    "                    if dist > 50:\n",
    "                        paths_to_clients.append(None)\n",
    "                        statuses.append(2)\n",
    "                    else:    \n",
    "                        path, status = search(maze, 1, (car_x, car_y), (x ,y))\n",
    "                        paths_to_clients.append(path)\n",
    "                        statuses.append(status)\n",
    "\n",
    "                completed_paths = [p for p,s in zip(paths_to_clients, statuses) if s==0]\n",
    "                if len(completed_paths)>0:\n",
    "                    min_path = min(completed_paths, key = lambda p: len(p))\n",
    "                else:\n",
    "                    min_ind = np.argmin(np.array(customer_dists))\n",
    "                    min_path = paths_to_clients[min_ind]\n",
    "\n",
    "                self.current_target = min_path[-1]\n",
    "\n",
    "                #current_target, path = min(zip(customer_dists, paths_to_clients), key = lambda p: len(p[1]))\n",
    "                target_cell = min_path[0]\n",
    "            else:\n",
    "                x, y = self.current_target\n",
    "                path, _ = search(maze, 1, (car_x, car_y), (x ,y))\n",
    "                if not path:\n",
    "                    self.current_target = None\n",
    "                    return 4\n",
    "                target_cell = path[0]\n",
    "                if len(path)==1:\n",
    "                    self.current_target = None\n",
    "            #current_path = current_path[1:] if len(current_path) > 1 else None\n",
    "\n",
    "            #target_cell = path[0] if path is not None else None\n",
    "\n",
    "        if target_cell is None:\n",
    "            return 4\n",
    "        if target_cell[0] == car_x and target_cell[1] == car_y - 1:\n",
    "            #return 3\n",
    "            return 3\n",
    "        if target_cell[0] == car_x and target_cell[1] == car_y + 1:\n",
    "            #return 1\n",
    "            return 1\n",
    "        if target_cell[0] == car_x - 1 and target_cell[1] == car_y:\n",
    "            #return 2\n",
    "            return 2\n",
    "        if target_cell[0] == car_x + 1 and target_cell[1] == car_y:\n",
    "            #return 0\n",
    "            return 0\n",
    "        return 4\n",
    "    \n",
    "def get_teacher_action(currentTarget, observation):\n",
    "    \"\"\" a wrapper for the megaalg \"\"\"\n",
    "    return megaalg(currentTarget, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from client import CarDirection, Client\n",
    "from env import JunctionEnvironment\n",
    "from time import sleep\n",
    "\n",
    "# https://github.com/avisingh599/imitation-dagger/blob/master/dagger.py\n",
    "team_name = \"ipa\"\n",
    "team_key = \"admin\"\n",
    "# aggregate and retrain\n",
    "dagger_itr = 5\n",
    "\n",
    "DAGGER_LEARNING_DATASET_DIR = os.path.join(DATASET_DIR, \"dagger_learning\")\n",
    "\n",
    "observations_all_dagger = observations_all\n",
    "actions_all_dagger = actions_all\n",
    "\n",
    "dagger_experiment = np.random.randint(0, 100000)\n",
    "for itr in range(dagger_itr):\n",
    "    print(\"begin dagger..\")\n",
    "    ob_list = []\n",
    "    teacher_actions = []\n",
    "    \n",
    "    car_id = '0' # TODO: is it ok to set is to '0' always?\n",
    "    currentTarget = CurrentTarget()\n",
    "    client = Client(team_name=team_name, team_key=team_key)\n",
    "    env = JunctionEnvironment(client)\n",
    "    ob = env.reset()[car_id]\n",
    "    score_sum = 0.0\n",
    "      \n",
    "    sleep(0.5)\n",
    "    \n",
    "    print(\"make obs with dagger..\")\n",
    "    for i in range(steps):\n",
    "        ob_padded = center_pad_observations(ob[np.newaxis,:,:,:], receptor_size=100)\n",
    "        act_pred = model.predict(ob_padded)\n",
    "        act = np.argmax(act_pred)\n",
    "        act_teach = get_teacher_action(currentTarget, ob) # most probably the teacher does not depend on the padded observation\n",
    "        ob, score, done, _ = env.step(act, car_id) #env.step(act)\n",
    "        if done is True:\n",
    "            break\n",
    "        else:\n",
    "            ob_list.append(ob_padded[0,:,:,:])\n",
    "            teacher_actions.append(act_teach)\n",
    "        score_sum += score\n",
    "        print(i, score, score_sum, done, act)\n",
    "    print('Episode done ', itr, i, score_sum)\n",
    "    # output_file.write('Number of Steps: %02d\\t Score: %0.04f\\n'%(i, score_sum))\n",
    "    env.close()\n",
    "\n",
    "    # TODO: what is this for?\n",
    "    # if i==(steps-1):\n",
    "    #    break\n",
    "    observations_all_dagger = np.append(observations_all_dagger, np.array(ob_list), axis=0)\n",
    "    actions_all_dagger = np.append(actions_all_dagger, teacher_actions, axis=0)\n",
    "\n",
    "    print(\"train on dagger..\")\n",
    "    \n",
    "    ###{{{ TensorBoard Logging and CheckPoint Callbacks begin\n",
    "    tb_cb = TensorBoard(log_dir='./logs/dagger_{0}'.format(dagger_experiment), histogram_freq=0, batch_size=batch_size, \n",
    "                        write_graph=True, write_grads=False, write_images=False, \n",
    "                        embeddings_freq=0, embeddings_layer_names=None, \n",
    "                        embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "    checkpoint_path=\"checkpoints/dagger/weights-improvement-{epoch:02d}-{val_loss:.2f}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    cp_cb = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "    load_checkpoint = '' # example: 'weights-improvement-01-1.50.ckpt'\n",
    "\n",
    "    if load_checkpoint != '':\n",
    "        load_path = os.path.join(checkpoint_dir, load_checkpoint)\n",
    "        print(\"loading %s \" % load_path)\n",
    "        model.load_weights(load_path)\n",
    "    ###}}} TensorBoard Logging and CheckPoint Callbacks Ends\n",
    "\n",
    "    model.fit(observations_all_dagger, actions_all_dagger,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.2,\n",
    "              nb_epoch=1,#nb_epoch,\n",
    "              shuffle=True,\n",
    "              callbacks=[tb_cb, cp_cb])\n",
    "    \n",
    "print(\"Saving dagger generated data..\")\n",
    "if not os.path.exists(DAGGER_LEARNING_DATASET_DIR):\n",
    "    os.makedirs(DAGGER_LEARNING_DATASET_DIR)\n",
    "\n",
    "with open(os.path.join(DAGGER_LEARNING_DATASET_DIR, f\"game_{dagger_experiment}_car_{car_id}_obs.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(observations_all_dagger, f)\n",
    "    \n",
    "with open(os.path.join(DAGGER_LEARNING_DATASET_DIR, f\"game_{dagger_experiment}_car_{car_id}_actions.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(actions_all_dagger, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
